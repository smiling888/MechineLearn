将多种机器学习算法组合在一起，使计算出来的结果更好
集成算法是提高算法准确度的有效方法之一，本章就介绍如何通过scikit-learn来实现集成算法。本章将会介绍以下几种算法：装袋（Bagging）算法。提升（Boosting）算法。投票（Voting）算法。

- 装袋（Bagging）算法：先将训练集分离成多个子集，然后通过各个子集训练多个模型。
- 提升（Boosting）算法：训练多个模型并组成一个序列，序列中的每一个模型都会修正前一个模型的错误。
- 投票（Voting）算法：训练多个模型，并采用样本统计来提高模型的准确度。

## 装袋算法

装袋决策树（Bagged Decision Trees）。随机森林（Random Forest）。极端随机树（Extra Trees）。

极端随机树与随机森林有两个主要的区别：
（1）随机森林应用的是Bagging模型，而极端随机树是使用所有的训练样本得到每棵决策树，也就是每棵决策树应用的是相同的全部训练样本。
（2）随机森林是在一个随机子集内得到最优分叉特征属性，而极端随机树是完全随机地选择分叉特征属性，从而实现对决策树进行分叉的。

## 提升算法
- AdaBoost。
- 随机梯度提升（Stochastic Gradient Boosting）。