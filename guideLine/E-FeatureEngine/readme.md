## 四个特征选择方法
通过选定数据特征来训练算法，得到一个能够提高准确度的模型

- 单变量特征选定。
 SelectKBest是对卡方检验的实现，可以使用一系列统计方法来选定数据特征。SelectKBest不仅可以执行卡方检验来选择数据特征，还可以通过相关系数、互信息法等统计方法来选定数据特征
 卡方检验说明：https://zhuanlan.zhihu.com/p/69888032
 大数据运营场景中，通常用在某个变量(或特征)值是不是和应变量有显著关系。

- 递归特征消除。
    RFE-使用一个基模型来进行多轮训练，每轮训练后消除若干权值系数的特征，再基于新的特征集进行下一轮训练。通过每一个基模型的精度，找到对最终的预测结果影响最大的数据特征。

- 主要成分分析。

主要成分分析（PCA）是使用线性代数来转换压缩数据，通常被称作数据降维。常见的降维方法除了主要成分分析（PCA），还有线性判别分析（LDA）
其本质是将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。
PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法

- 特征的重要性。
袋装决策树算法（Bagged Decision Tress）、随机森林算法和极端随机树算法都可以用来计算数据特征的重要性
ExtraTreesClassifier类进行特征的重要性计算的例子

## 目的
降低数据的拟合度：较少的冗余数据，会使算法得出结论的机会更大。
提高算法精度：较少的误导数据，能够提高算法的准确度。
减少训练时间：越少的数据，训练模型所需要的时间越少。

文档：（http://scikit-learn.org/stable/modules/feature_selection.html）